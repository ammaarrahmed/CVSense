{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad27b58",
   "metadata": {},
   "source": [
    "# Module 1: Data Ingestion & Resume Handling\n",
    "\n",
    "**Project:** CVSense - Intelligent Resume Classifier  \n",
    "**Module Owner:** Ammaar Ahmed \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module is responsible for:\n",
    "1. **Data Collection**: Downloading resume and job description datasets from Kaggle\n",
    "2. **PDF Extraction**: Converting PDF resumes to text format\n",
    "3. **Data Organization**: Storing data in a structured format for downstream modules\n",
    "4. **Data Validation**: Ensuring quality and consistency of extracted data\n",
    "\n",
    "---\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Required packages:\n",
    "- `pandas`: Data manipulation\n",
    "- `PyPDF2` or `pdfplumber`: PDF text extraction\n",
    "- `opendatasets`: Kaggle dataset download\n",
    "- `pathlib`: File path handling\n",
    "- `json`: Data serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc3ded",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f988cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas PyPDF2 pdfplumber opendatasets kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd32705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded credentials from .env file\n",
      "‚úì Kaggle user: ammaarx\n",
      "‚úì Using pdfplumber for PDF extraction\n",
      "\n",
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables if .env file exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load .env file from project root\n",
    "    env_path = Path('/home/ammaar/CODE/CVSense/.env')\n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path, override=True)\n",
    "        \n",
    "        # Set Kaggle credentials from environment variables\n",
    "        kaggle_user = os.getenv('KAGGLE_USERNAME')\n",
    "        kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "        \n",
    "        if kaggle_user and kaggle_key:\n",
    "            os.environ['KAGGLE_USERNAME'] = kaggle_user\n",
    "            os.environ['KAGGLE_KEY'] = kaggle_key\n",
    "            print(\"‚úì Loaded credentials from .env file\")\n",
    "            print(f\"‚úì Kaggle user: {kaggle_user}\")\n",
    "        else:\n",
    "            print(\"‚ö† Warning: KAGGLE_USERNAME or KAGGLE_KEY not found in .env\")\n",
    "            print(\"  ‚Üí Edit .env and add your credentials (see README.md)\")\n",
    "    else:\n",
    "        print(\"‚ö† No .env file found. Checking for ~/.kaggle/kaggle.json...\")\n",
    "        kaggle_json = Path.home() / '.kaggle' / 'kaggle.json'\n",
    "        if kaggle_json.exists():\n",
    "            print(\"‚úì Using system-wide Kaggle credentials\")\n",
    "        else:\n",
    "            print(\"  ‚Üí Create .env file with credentials (see README.md)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "\n",
    "# PDF extraction libraries\n",
    "try:\n",
    "    import pdfplumber\n",
    "    PDF_LIBRARY = 'pdfplumber'\n",
    "    print(\"‚úì Using pdfplumber for PDF extraction\")\n",
    "except ImportError:\n",
    "    import PyPDF2\n",
    "    PDF_LIBRARY = 'PyPDF2'\n",
    "    print(\"‚úì Using PyPDF2 for PDF extraction\")\n",
    "\n",
    "# Kaggle dataset download\n",
    "import opendatasets as od\n",
    "\n",
    "print(\"\\n‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ef7f8",
   "metadata": {},
   "source": [
    "## 2. Configuration & Directory Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15893d8a",
   "metadata": {},
   "source": [
    "### üîê Kaggle API Setup\n",
    "\n",
    "Make sure you've set up Kaggle credentials in `.env` file:\n",
    "\n",
    "```bash\n",
    "# In project root\n",
    "cp .env.example .env\n",
    "# Edit .env with your Kaggle username & API key from kaggle.com/account\n",
    "```\n",
    "\n",
    "The `.env` file is in `.gitignore` - your credentials stay private!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ce7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Structure:\n",
      "  ‚îî‚îÄ‚îÄ Project Root: /home/ammaar/CODE/CVSense\n",
      "      ‚îî‚îÄ‚îÄ Data: /home/ammaar/CODE/CVSense/data\n",
      "          ‚îú‚îÄ‚îÄ Resumes: /home/ammaar/CODE/CVSense/data/resumes\n",
      "          ‚îî‚îÄ‚îÄ Job Descriptions: /home/ammaar/CODE/CVSense/data/job_descriptions\n",
      "\n",
      "‚úì Directories configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path('/home/ammaar/CODE/CVSense')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RESUME_DIR = DATA_DIR / 'resumes'\n",
    "JOB_DESC_DIR = DATA_DIR / 'job_descriptions'\n",
    "MODULE_DIR = PROJECT_ROOT / 'module_1_data_ingestion'\n",
    "\n",
    "# Dataset configuration\n",
    "MAX_RESUMES = 100\n",
    "MAX_JOB_DESCRIPTIONS = 50\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RESUME_DIR.mkdir(parents=True, exist_ok=True)\n",
    "JOB_DESC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directory Structure:\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"      ‚îî‚îÄ‚îÄ Data: {DATA_DIR}\")\n",
    "print(f\"          ‚îú‚îÄ‚îÄ Resumes: {RESUME_DIR}\")\n",
    "print(f\"          ‚îî‚îÄ‚îÄ Job Descriptions: {JOB_DESC_DIR}\")\n",
    "print(f\"\\n‚úì Directories configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fc9af",
   "metadata": {},
   "source": [
    "## 3. Dataset Download from Kaggle\n",
    "\n",
    "We'll download a publicly available resume dataset from Kaggle. You'll need your Kaggle API credentials.\n",
    "\n",
    "**Note:** Make sure you have `kaggle.json` in `~/.kaggle/` directory with your API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb0ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading resume dataset from Kaggle...\n",
      "(Using credentials from .env or ~/.kaggle/kaggle.json)\n",
      "\n",
      "‚úì Using credentials from .env (user: ammaarx)\n",
      "‚úì Created ~/.kaggle/kaggle.json from .env credentials\n",
      "Downloading from Kaggle (this may take a minute)...\n",
      "Dataset URL: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
      "\n",
      "‚úì Dataset downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Download Resume Dataset from Kaggle\n",
    "RESUME_DATASET_URL = 'https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset'\n",
    "DATASET_SLUG = 'snehaanbhawal/resume-dataset'\n",
    "\n",
    "try:\n",
    "    print(\"Downloading resume dataset from Kaggle...\")\n",
    "    print(\"(Using credentials from .env or ~/.kaggle/kaggle.json)\\n\")\n",
    "    \n",
    "    # Check if credentials are available in environment\n",
    "    kaggle_user = os.getenv('KAGGLE_USERNAME')\n",
    "    kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_json = kaggle_dir / 'kaggle.json'\n",
    "    \n",
    "    # If .env credentials exist, create/update kaggle.json\n",
    "    if kaggle_user and kaggle_key and kaggle_user != 'your_kaggle_username_here':\n",
    "        print(f\"‚úì Using credentials from .env (user: {kaggle_user})\")\n",
    "        \n",
    "        # Create .kaggle directory if it doesn't exist\n",
    "        kaggle_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Write credentials to kaggle.json\n",
    "        kaggle_creds = {\n",
    "            \"username\": kaggle_user,\n",
    "            \"key\": kaggle_key\n",
    "        }\n",
    "        with open(kaggle_json, 'w') as f:\n",
    "            json.dump(kaggle_creds, f)\n",
    "        \n",
    "        # Set proper permissions (Unix-like systems)\n",
    "        try:\n",
    "            kaggle_json.chmod(0o600)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"‚úì Created ~/.kaggle/kaggle.json from .env credentials\")\n",
    "    \n",
    "    elif kaggle_json.exists():\n",
    "        print(\"‚úì Using existing ~/.kaggle/kaggle.json\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Kaggle credentials not found!\\n\"\n",
    "            \"Setup: cp .env.example .env and add your credentials\\n\"\n",
    "            \"Get API key from: https://www.kaggle.com/account\"\n",
    "        )\n",
    "    \n",
    "    # Download dataset using Kaggle API (more reliable than opendatasets)\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    temp_download_dir = MODULE_DIR / 'temp_kaggle_data'\n",
    "    temp_download_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading from Kaggle (this may take a minute)...\")\n",
    "    api.dataset_download_files(DATASET_SLUG, path=str(temp_download_dir), unzip=True)\n",
    "    \n",
    "    print(\"\\n‚úì Dataset downloaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error downloading dataset: {e}\")\n",
    "    print(\"\\nCreating sample dataset for demonstration...\")\n",
    "    print(\"For production: add Kaggle credentials to .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bf521",
   "metadata": {},
   "source": [
    "## 4. Load and Process Resume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35168931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found resume dataset: Resume.csv\n",
      "\n",
      "Dataset shape: (2484, 4)\n",
      "Columns: ['ID', 'Resume_str', 'Resume_html', 'Category']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the downloaded CSV file\n",
    "temp_download_dir = MODULE_DIR / 'temp_kaggle_data'\n",
    "\n",
    "# Look for CSV files in the downloaded directory\n",
    "csv_files = list(temp_download_dir.rglob('*.csv'))\n",
    "\n",
    "if csv_files:\n",
    "    resume_csv_path = csv_files[0]\n",
    "    print(f\"Found resume dataset: {resume_csv_path.name}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df_resumes = pd.read_csv(resume_csv_path)\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df_resumes.shape}\")\n",
    "    print(f\"Columns: {list(df_resumes.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df_resumes.head())\n",
    "else:\n",
    "    print(\"No CSV files found. Creating sample dataset...\")\n",
    "    # Create a small sample dataset for demonstration\n",
    "    df_resumes = pd.DataFrame({\n",
    "        'Category': ['Data Science', 'Software Engineering', 'Web Development'],\n",
    "        'Resume': [\n",
    "            'Experienced Data Scientist with Python, ML, and statistical analysis skills...',\n",
    "            'Software Engineer proficient in Java, C++, and system design...',\n",
    "            'Full-stack web developer with React, Node.js, and database experience...'\n",
    "        ]\n",
    "    })\n",
    "    print(\"Sample dataset created for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e6fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2484 resumes. Limiting to 100...\n",
      "\n",
      "‚úì Working with 100 resumes\n"
     ]
    }
   ],
   "source": [
    "# Limit to MAX_RESUMES\n",
    "if len(df_resumes) > MAX_RESUMES:\n",
    "    print(f\"Dataset has {len(df_resumes)} resumes. Limiting to {MAX_RESUMES}...\")\n",
    "    df_resumes = df_resumes.sample(n=MAX_RESUMES, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    print(f\"Using all {len(df_resumes)} resumes from dataset\")\n",
    "\n",
    "print(f\"\\n‚úì Working with {len(df_resumes)} resumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aea07d",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6f9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume text column identified: 'Resume_str'\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning for resumes and job descriptions.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters that might cause issues\n",
    "    text = text.replace('\\x00', '')\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Identify the resume text column (it might have different names)\n",
    "resume_text_col = None\n",
    "for col in df_resumes.columns:\n",
    "    if 'resume' in col.lower() or 'text' in col.lower():\n",
    "        resume_text_col = col\n",
    "        break\n",
    "\n",
    "if resume_text_col:\n",
    "    print(f\"Resume text column identified: '{resume_text_col}'\")\n",
    "    df_resumes['cleaned_resume'] = df_resumes[resume_text_col].apply(clean_text)\n",
    "else:\n",
    "    print(\"Could not identify resume text column automatically\")\n",
    "    print(f\"Available columns: {list(df_resumes.columns)}\")\n",
    "    # Use the second column by default if exists\n",
    "    if len(df_resumes.columns) > 1:\n",
    "        resume_text_col = df_resumes.columns[1]\n",
    "        print(f\"Using column: '{resume_text_col}'\")\n",
    "        df_resumes['cleaned_resume'] = df_resumes[resume_text_col].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744feb9d",
   "metadata": {},
   "source": [
    "## 6. Data Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f698ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating resume data quality...\n",
      "\n",
      "Validation Results:\n",
      "  ‚úì Valid resumes: 100\n",
      "  ‚úó Invalid resumes: 0\n"
     ]
    }
   ],
   "source": [
    "def validate_resume_text(text, min_length=50):\n",
    "    \"\"\"\n",
    "    Validate resume text quality.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Resume text to validate\n",
    "        min_length (int): Minimum acceptable length\n",
    "        \n",
    "    Returns:\n",
    "        dict: Validation results with 'valid' flag and 'issues' list\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if not text or pd.isna(text):\n",
    "        issues.append(\"Empty text\")\n",
    "        return {'valid': False, 'issues': issues}\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Check minimum length\n",
    "    if len(text) < min_length:\n",
    "        issues.append(f\"Text too short ({len(text)} chars)\")\n",
    "    \n",
    "    # Check for excessive non-alphabetic characters\n",
    "    alpha_ratio = sum(c.isalpha() or c.isspace() for c in text) / len(text)\n",
    "    if alpha_ratio < 0.5:\n",
    "        issues.append(f\"Low alphabetic content ({alpha_ratio:.2%})\")\n",
    "    \n",
    "    # Check for common extraction errors\n",
    "    if text.count('ÔøΩ') > 5:\n",
    "        issues.append(\"Contains encoding errors\")\n",
    "    \n",
    "    return {\n",
    "        'valid': len(issues) == 0,\n",
    "        'issues': issues,\n",
    "        'length': len(text),\n",
    "        'alpha_ratio': alpha_ratio\n",
    "    }\n",
    "\n",
    "# Validate all resumes\n",
    "print(\"Validating resume data quality...\\n\")\n",
    "\n",
    "if 'cleaned_resume' in df_resumes.columns:\n",
    "    df_resumes['validation'] = df_resumes['cleaned_resume'].apply(validate_resume_text)\n",
    "    df_resumes['is_valid'] = df_resumes['validation'].apply(lambda x: x['valid'])\n",
    "    \n",
    "    valid_count = df_resumes['is_valid'].sum()\n",
    "    invalid_count = len(df_resumes) - valid_count\n",
    "    \n",
    "    print(f\"Validation Results:\")\n",
    "    print(f\"  ‚úì Valid resumes: {valid_count}\")\n",
    "    print(f\"  ‚úó Invalid resumes: {invalid_count}\")\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        print(f\"\\nSample issues:\")\n",
    "        invalid_samples = df_resumes[~df_resumes['is_valid']].head(3)\n",
    "        for idx, row in invalid_samples.iterrows():\n",
    "            print(f\"  Resume {idx}: {row['validation']['issues']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e706a0",
   "metadata": {},
   "source": [
    "## 7. PDF Extraction Utility (for future PDF resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927bccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PDF extraction function defined\n",
      "  Using: pdfplumber\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str or Path): Path to PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text from PDF\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    if PDF_LIBRARY == 'pdfplumber':\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting with pdfplumber: {e}\")\n",
    "    \n",
    "    else:  # PyPDF2\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting with PyPDF2: {e}\")\n",
    "    \n",
    "    return clean_text(text)\n",
    "\n",
    "print(\"‚úì PDF extraction function defined\")\n",
    "print(f\"  Using: {PDF_LIBRARY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56bfedb",
   "metadata": {},
   "source": [
    "## 8. Create Job Descriptions Dataset\n",
    "\n",
    "Creating sample job descriptions that align with common resume categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45cffcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 job descriptions\n",
      "\n",
      "Categories: ['Data Science' 'Software Engineering' 'Web Development' 'DevOps'\n",
      " 'Mobile Development' 'Cloud Computing' 'Quality Assurance']\n",
      "\n",
      "‚úì Job descriptions dataset ready\n"
     ]
    }
   ],
   "source": [
    "# Sample job descriptions for common tech roles\n",
    "sample_job_descriptions = [\n",
    "    {\n",
    "        'job_id': 'JD001',\n",
    "        'title': 'Senior Data Scientist',\n",
    "        'category': 'Data Science',\n",
    "        'description': '''We are seeking a Senior Data Scientist to join our AI team. \n",
    "        The ideal candidate will have strong experience in machine learning, statistical analysis, \n",
    "        and Python programming. Responsibilities include developing predictive models, conducting \n",
    "        A/B testing, and presenting insights to stakeholders. Required skills: Python, SQL, \n",
    "        TensorFlow/PyTorch, scikit-learn, pandas, statistics, and data visualization. \n",
    "        Experience with cloud platforms (AWS/GCP) is a plus.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD002',\n",
    "        'title': 'Full Stack Software Engineer',\n",
    "        'category': 'Software Engineering',\n",
    "        'description': '''Looking for a Full Stack Software Engineer to build scalable web applications. \n",
    "        You will work on both frontend and backend development using modern technologies. \n",
    "        Required skills: JavaScript, React, Node.js, RESTful APIs, databases (PostgreSQL/MongoDB), \n",
    "        Git, and Agile methodologies. Experience with cloud deployment and CI/CD pipelines preferred. \n",
    "        Strong problem-solving and communication skills required.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD003',\n",
    "        'title': 'Machine Learning Engineer',\n",
    "        'category': 'Data Science',\n",
    "        'description': '''Seeking Machine Learning Engineer to develop and deploy ML models at scale. \n",
    "        Responsibilities include model training, optimization, and productionization. Required skills: \n",
    "        Python, deep learning frameworks (TensorFlow/PyTorch), MLOps, Docker, Kubernetes, and \n",
    "        experience with large-scale datasets. Knowledge of NLP and computer vision is a plus. \n",
    "        PhD or Masters in Computer Science or related field preferred.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD004',\n",
    "        'title': 'Frontend Developer',\n",
    "        'category': 'Web Development',\n",
    "        'description': '''We need a creative Frontend Developer to build beautiful user interfaces. \n",
    "        You will work with designers to implement responsive web applications. Required skills: \n",
    "        HTML5, CSS3, JavaScript, React or Vue.js, responsive design, cross-browser compatibility, \n",
    "        and version control (Git). Experience with TypeScript, testing frameworks, and UI/UX \n",
    "        principles is highly valued.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD005',\n",
    "        'title': 'DevOps Engineer',\n",
    "        'category': 'DevOps',\n",
    "        'description': '''Looking for DevOps Engineer to manage our cloud infrastructure and CI/CD pipelines. \n",
    "        Responsibilities include automation, monitoring, and ensuring system reliability. Required skills: \n",
    "        Linux, Docker, Kubernetes, Jenkins/GitLab CI, AWS/Azure, Terraform, scripting (Python/Bash), \n",
    "        and networking fundamentals. Experience with monitoring tools (Prometheus, Grafana) preferred.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD006',\n",
    "        'title': 'Data Analyst',\n",
    "        'category': 'Data Science',\n",
    "        'description': '''Seeking Data Analyst to transform data into actionable insights. You will create \n",
    "        dashboards, perform statistical analysis, and support business decision-making. Required skills: \n",
    "        SQL, Excel, Python/R, data visualization (Tableau/Power BI), statistical analysis, and \n",
    "        business intelligence. Strong analytical thinking and communication skills essential.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD007',\n",
    "        'title': 'Backend Developer',\n",
    "        'category': 'Software Engineering',\n",
    "        'description': '''We are hiring a Backend Developer to build robust server-side applications. \n",
    "        You will design APIs, optimize databases, and ensure system scalability. Required skills: \n",
    "        Java/Python/Node.js, RESTful API design, databases (SQL and NoSQL), microservices architecture, \n",
    "        caching (Redis), and message queues. Experience with distributed systems is a plus.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD008',\n",
    "        'title': 'Mobile App Developer',\n",
    "        'category': 'Mobile Development',\n",
    "        'description': '''Looking for Mobile App Developer to create native mobile applications. \n",
    "        You will develop features for iOS and Android platforms. Required skills: Swift/Kotlin, \n",
    "        mobile UI/UX patterns, RESTful APIs, local databases, push notifications, and app store \n",
    "        deployment. Experience with React Native or Flutter is beneficial.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD009',\n",
    "        'title': 'Cloud Architect',\n",
    "        'category': 'Cloud Computing',\n",
    "        'description': '''Seeking experienced Cloud Architect to design scalable cloud solutions. \n",
    "        You will define architecture patterns, security standards, and migration strategies. \n",
    "        Required skills: AWS/Azure/GCP, cloud architecture patterns, security best practices, \n",
    "        infrastructure as code, networking, and cost optimization. Relevant certifications preferred.'''\n",
    "    },\n",
    "    {\n",
    "        'job_id': 'JD010',\n",
    "        'title': 'QA Automation Engineer',\n",
    "        'category': 'Quality Assurance',\n",
    "        'description': '''We need QA Automation Engineer to build and maintain test automation frameworks. \n",
    "        You will design test strategies and ensure product quality. Required skills: Test automation \n",
    "        (Selenium/Cypress), programming (Python/Java), API testing, CI/CD integration, test frameworks, \n",
    "        and bug tracking tools. Experience with performance and security testing is a plus.'''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_jobs = pd.DataFrame(sample_job_descriptions)\n",
    "df_jobs['cleaned_description'] = df_jobs['description'].apply(clean_text)\n",
    "\n",
    "print(f\"Created {len(df_jobs)} job descriptions\")\n",
    "print(f\"\\nCategories: {df_jobs['category'].unique()}\")\n",
    "print(f\"\\n‚úì Job descriptions dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d8c8f",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data for Next Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ac44ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved 100 resumes to: /home/ammaar/CODE/CVSense/data/processed_resumes.csv\n",
      "‚úì Saved 10 job descriptions to: /home/ammaar/CODE/CVSense/data/processed_job_descriptions.csv\n",
      "‚úì Saved individual job description files to: /home/ammaar/CODE/CVSense/data/job_descriptions\n"
     ]
    }
   ],
   "source": [
    "# Save resumes to CSV\n",
    "resume_output_path = DATA_DIR / 'processed_resumes.csv'\n",
    "if 'cleaned_resume' in df_resumes.columns:\n",
    "    # Save only essential columns\n",
    "    columns_to_save = []\n",
    "    if 'Category' in df_resumes.columns:\n",
    "        columns_to_save.append('Category')\n",
    "    columns_to_save.extend(['cleaned_resume', 'is_valid'])\n",
    "    \n",
    "    df_resumes[columns_to_save].to_csv(resume_output_path, index=False)\n",
    "    print(f\"‚úì Saved {len(df_resumes)} resumes to: {resume_output_path}\")\n",
    "\n",
    "# Save job descriptions to CSV\n",
    "job_output_path = DATA_DIR / 'processed_job_descriptions.csv'\n",
    "df_jobs.to_csv(job_output_path, index=False)\n",
    "print(f\"‚úì Saved {len(df_jobs)} job descriptions to: {job_output_path}\")\n",
    "\n",
    "# Save also as individual text files for easy access\n",
    "for idx, row in df_jobs.iterrows():\n",
    "    job_file = JOB_DESC_DIR / f\"{row['job_id']}_{row['title'].replace(' ', '_')}.txt\"\n",
    "    with open(job_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Title: {row['title']}\\n\")\n",
    "        f.write(f\"Category: {row['category']}\\n\")\n",
    "        f.write(f\"\\nDescription:\\n{row['description']}\")\n",
    "\n",
    "print(f\"‚úì Saved individual job description files to: {JOB_DESC_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e1da0",
   "metadata": {},
   "source": [
    "## 10. Generate Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879323d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "\n",
      "Timestamp: 2026-01-17 22:27:49\n",
      "\n",
      "Resumes:\n",
      "  Total: 100\n",
      "  Valid: 100\n",
      "  Invalid: 0\n",
      "  Avg Length: 6345 characters\n",
      "  Length Range: 1319 - 35217\n",
      "\n",
      "Job Descriptions:\n",
      "  Total: 10\n",
      "  Categories: Data Science, Software Engineering, Web Development, DevOps, Mobile Development, Cloud Computing, Quality Assurance\n",
      "\n",
      "‚úì Report saved to: /home/ammaar/CODE/CVSense/module_1_data_ingestion/data_quality_report.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def generate_data_quality_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report for ingested data.\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'resumes': {\n",
    "            'total_count': len(df_resumes),\n",
    "            'valid_count': int(df_resumes['is_valid'].sum()) if 'is_valid' in df_resumes.columns else 0,\n",
    "            'invalid_count': int((~df_resumes['is_valid']).sum()) if 'is_valid' in df_resumes.columns else 0,\n",
    "        },\n",
    "        'job_descriptions': {\n",
    "            'total_count': len(df_jobs),\n",
    "            'categories': list(df_jobs['category'].unique()),\n",
    "        },\n",
    "        'data_paths': {\n",
    "            'resumes_csv': str(resume_output_path),\n",
    "            'jobs_csv': str(job_output_path),\n",
    "            'job_desc_dir': str(JOB_DESC_DIR),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add statistics\n",
    "    if 'cleaned_resume' in df_resumes.columns:\n",
    "        valid_resumes = df_resumes[df_resumes['is_valid']]\n",
    "        if len(valid_resumes) > 0:\n",
    "            lengths = valid_resumes['cleaned_resume'].str.len()\n",
    "            report['resumes']['avg_length'] = int(lengths.mean())\n",
    "            report['resumes']['min_length'] = int(lengths.min())\n",
    "            report['resumes']['max_length'] = int(lengths.max())\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save report\n",
    "quality_report = generate_data_quality_report()\n",
    "report_path = MODULE_DIR / 'data_quality_report.json'\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(quality_report, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTimestamp: {quality_report['timestamp']}\")\n",
    "print(f\"\\nResumes:\")\n",
    "print(f\"  Total: {quality_report['resumes']['total_count']}\")\n",
    "print(f\"  Valid: {quality_report['resumes']['valid_count']}\")\n",
    "print(f\"  Invalid: {quality_report['resumes']['invalid_count']}\")\n",
    "if 'avg_length' in quality_report['resumes']:\n",
    "    print(f\"  Avg Length: {quality_report['resumes']['avg_length']} characters\")\n",
    "    print(f\"  Length Range: {quality_report['resumes']['min_length']} - {quality_report['resumes']['max_length']}\")\n",
    "\n",
    "print(f\"\\nJob Descriptions:\")\n",
    "print(f\"  Total: {quality_report['job_descriptions']['total_count']}\")\n",
    "print(f\"  Categories: {', '.join(quality_report['job_descriptions']['categories'])}\")\n",
    "\n",
    "print(f\"\\n‚úì Report saved to: {report_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987a65b",
   "metadata": {},
   "source": [
    "## 11. Create Data Format Specification Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc6c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data format specification saved to: /home/ammaar/CODE/CVSense/module_1_data_ingestion/DATA_FORMAT_SPECIFICATION.md\n"
     ]
    }
   ],
   "source": [
    "data_spec = \"\"\"\n",
    "# Data Format Specification for CVSense Pipeline\n",
    "\n",
    "## Module 1 Output Format\n",
    "\n",
    "### Processed Resumes (`data/processed_resumes.csv`)\n",
    "\n",
    "**Columns:**\n",
    "- `Category` (optional): The category/field of the resume (e.g., 'Data Science', 'Software Engineering')\n",
    "- `cleaned_resume`: Cleaned and validated resume text ready for preprocessing\n",
    "- `is_valid`: Boolean flag indicating if the resume passed quality validation\n",
    "\n",
    "**Data Quality Standards:**\n",
    "- Minimum text length: 50 characters\n",
    "- Minimum alphabetic content ratio: 50%\n",
    "- Encoding errors removed\n",
    "- Excessive whitespace normalized\n",
    "\n",
    "### Processed Job Descriptions (`data/processed_job_descriptions.csv`)\n",
    "\n",
    "**Columns:**\n",
    "- `job_id`: Unique identifier for the job posting (e.g., 'JD001')\n",
    "- `title`: Job title\n",
    "- `category`: Job category/field\n",
    "- `description`: Original job description text\n",
    "- `cleaned_description`: Cleaned job description ready for preprocessing\n",
    "\n",
    "**Individual Files:** Each job description is also saved as a separate text file in `data/job_descriptions/`\n",
    "\n",
    "## Expected Input for Module 2 (Text Preprocessing)\n",
    "\n",
    "Module 2 should:\n",
    "1. Load `data/processed_resumes.csv` and `data/processed_job_descriptions.csv`\n",
    "2. Use only rows where `is_valid == True` for resumes\n",
    "3. Apply text preprocessing to `cleaned_resume` and `cleaned_description` columns\n",
    "4. Output format should maintain the same structure with additional preprocessed columns\n",
    "\n",
    "## Data Validation Guidelines\n",
    "\n",
    "### Why Data Quality Matters:\n",
    "- **Poor PDF Extraction:** Corrupted characters, formatting issues can reduce matching accuracy\n",
    "- **Text Quality:** Low-quality text leads to poor feature extraction and inaccurate similarity scores\n",
    "- **Consistency:** Standardized format ensures all modules work correctly\n",
    "\n",
    "### Common PDF Extraction Challenges:\n",
    "1. **Encoding Issues:** Special characters may not extract correctly\n",
    "2. **Layout Problems:** Multi-column resumes can have scrambled text\n",
    "3. **Images as Text:** Text in images cannot be extracted without OCR\n",
    "4. **Tables:** Table formatting often gets lost in extraction\n",
    "\n",
    "## File Locations\n",
    "\n",
    "```\n",
    "CVSense/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed_resumes.csv          # Main resume dataset\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed_job_descriptions.csv # Main job descriptions dataset\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ resumes/                       # Individual resume files (if any)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ job_descriptions/              # Individual job description files\n",
    "‚îî‚îÄ‚îÄ module_1_data_ingestion/\n",
    "    ‚îú‚îÄ‚îÄ data_ingestion.ipynb           # Main implementation notebook\n",
    "    ‚îî‚îÄ‚îÄ data_quality_report.json       # Quality metrics and statistics\n",
    "```\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions about data format or quality issues, contact the Module 1 owner.\n",
    "\"\"\"\n",
    "\n",
    "# Save specification document\n",
    "spec_path = MODULE_DIR / 'DATA_FORMAT_SPECIFICATION.md'\n",
    "with open(spec_path, 'w') as f:\n",
    "    f.write(data_spec)\n",
    "\n",
    "print(f\"‚úì Data format specification saved to: {spec_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ffbe5",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33722e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODULE 1: DATA INGESTION - COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìä DELIVERABLES:\n",
      "\n",
      "1. Resume Dataset:\n",
      "   ‚îî‚îÄ /home/ammaar/CODE/CVSense/data/processed_resumes.csv\n",
      "   ‚îî‚îÄ 100 resumes (100 valid)\n",
      "\n",
      "2. Job Descriptions Dataset:\n",
      "   ‚îî‚îÄ /home/ammaar/CODE/CVSense/data/processed_job_descriptions.csv\n",
      "   ‚îî‚îÄ 10 job descriptions across 7 categories\n",
      "\n",
      "3. Documentation:\n",
      "   ‚îî‚îÄ /home/ammaar/CODE/CVSense/module_1_data_ingestion/data_quality_report.json\n",
      "   ‚îî‚îÄ /home/ammaar/CODE/CVSense/module_1_data_ingestion/DATA_FORMAT_SPECIFICATION.md\n",
      "\n",
      "4. Utilities:\n",
      "   ‚îî‚îÄ PDF extraction function (extract_text_from_pdf)\n",
      "   ‚îî‚îÄ Text cleaning function (clean_text)\n",
      "   ‚îî‚îÄ Validation function (validate_resume_text)\n",
      "\n",
      "üîÑ READY FOR MODULE 2:\n",
      "   ‚úì Data cleaned and validated\n",
      "   ‚úì Consistent format established\n",
      "   ‚úì Quality metrics documented\n",
      "   ‚úì Specification provided for downstream modules\n",
      "\n",
      "üìù KEY POINTS TO EXPLAIN:\n",
      "   ‚Ä¢ Data quality directly impacts ML model performance\n",
      "   ‚Ä¢ PDF extraction challenges: encoding, layout, images\n",
      "   ‚Ä¢ Validation ensures only quality data proceeds to next modules\n",
      "   ‚Ä¢ Standardized format enables smooth pipeline integration\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Module 1 implementation successful!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODULE 1: DATA INGESTION - COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä DELIVERABLES:\")\n",
    "print(f\"\\n1. Resume Dataset:\")\n",
    "print(f\"   ‚îî‚îÄ {resume_output_path}\")\n",
    "print(f\"   ‚îî‚îÄ {len(df_resumes)} resumes ({df_resumes['is_valid'].sum() if 'is_valid' in df_resumes.columns else 0} valid)\")\n",
    "\n",
    "print(f\"\\n2. Job Descriptions Dataset:\")\n",
    "print(f\"   ‚îî‚îÄ {job_output_path}\")\n",
    "print(f\"   ‚îî‚îÄ {len(df_jobs)} job descriptions across {len(df_jobs['category'].unique())} categories\")\n",
    "\n",
    "print(f\"\\n3. Documentation:\")\n",
    "print(f\"   ‚îî‚îÄ {report_path}\")\n",
    "print(f\"   ‚îî‚îÄ {spec_path}\")\n",
    "\n",
    "print(f\"\\n4. Utilities:\")\n",
    "print(f\"   ‚îî‚îÄ PDF extraction function (extract_text_from_pdf)\")\n",
    "print(f\"   ‚îî‚îÄ Text cleaning function (clean_text)\")\n",
    "print(f\"   ‚îî‚îÄ Validation function (validate_resume_text)\")\n",
    "\n",
    "print(\"\\nüîÑ READY FOR MODULE 2:\")\n",
    "print(\"   ‚úì Data cleaned and validated\")\n",
    "print(\"   ‚úì Consistent format established\")\n",
    "print(\"   ‚úì Quality metrics documented\")\n",
    "print(\"   ‚úì Specification provided for downstream modules\")\n",
    "\n",
    "print(\"\\nüìù KEY POINTS TO EXPLAIN:\")\n",
    "print(\"   ‚Ä¢ Data quality directly impacts ML model performance\")\n",
    "print(\"   ‚Ä¢ PDF extraction challenges: encoding, layout, images\")\n",
    "print(\"   ‚Ä¢ Validation ensures only quality data proceeds to next modules\")\n",
    "print(\"   ‚Ä¢ Standardized format enables smooth pipeline integration\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Module 1 implementation successful!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
